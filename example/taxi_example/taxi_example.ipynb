{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, json\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "from datamart.augment import Augment\n",
    "from datamart.utilities.utils import Utils\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_index = \"datamart_all\"\n",
    "\n",
    "augment = Augment(es_index=es_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a sub set about taxi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    d3mIndex tpep_pickup_datetime  num_pickups\n0          0  2018-01-01 00:00:00           67\n1          1  2018-01-01 01:00:00            8\n2          2  2018-01-01 02:00:00            0\n3          3  2018-01-01 03:00:00            0\n4          4  2018-01-01 04:00:00            7\n5          5  2018-01-01 05:00:00           10\n6          6  2018-01-01 06:00:00            9\n7          7  2018-01-01 07:00:00           28\n8          8  2018-01-01 08:00:00          157\n9          9  2018-01-01 09:00:00          259\n10        10  2018-01-01 10:00:00          301\n11        11  2018-01-01 11:00:00          436\n12        12  2018-01-01 12:00:00          369\n13        13  2018-01-01 13:00:00          347\n14        14  2018-01-01 14:00:00          494\n15        15  2018-01-01 15:00:00          544\n16        16  2018-01-01 16:00:00          467\n17        17  2018-01-01 17:00:00          690\n18        18  2018-01-01 18:00:00          461\n19        19  2018-01-01 19:00:00          465\n20        20  2018-01-01 20:00:00          451\n21        21  2018-01-01 21:00:00          556\n22        22  2018-01-01 22:00:00          570\n23        23  2018-01-01 23:00:00          499\n24        24  2018-01-02 00:00:00          399\n25        25  2018-01-02 01:00:00           13\n26        26  2018-01-02 02:00:00           17\n27        27  2018-01-02 03:00:00            1\n28        28  2018-01-02 04:00:00            1\n29        29  2018-01-02 05:00:00            8\n30        30  2018-01-02 06:00:00           18\n31        31  2018-01-02 07:00:00          203\n32        32  2018-01-02 08:00:00          359\n33        33  2018-01-02 09:00:00          641\n34        34  2018-01-02 10:00:00          447\n35        35  2018-01-02 11:00:00          598\n36        36  2018-01-02 12:00:00          425\n37        37  2018-01-02 13:00:00          475\n38        38  2018-01-02 14:00:00          572\n39        39  2018-01-02 15:00:00          432\n40        40  2018-01-02 16:00:00          526\n41        41  2018-01-02 17:00:00          577\n42        42  2018-01-02 18:00:00          573\n43        43  2018-01-02 19:00:00          537\n44        44  2018-01-02 20:00:00          464\n45        45  2018-01-02 21:00:00          549\n46        46  2018-01-02 22:00:00          491\n47        47  2018-01-02 23:00:00          394\n"
     ]
    }
   ],
   "source": [
    "old_df = pd.read_csv(\"./example/taxi_example/taxi.csv\")\n",
    "print(old_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query\n",
    "### Is there any dataset in Datamart has named_entity new york and is related to wind?\n",
    "### Besides, it covers the date from 2018-01-01 to 2018-01-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "metadatas = augment.query(\n",
    "    key_value_pairs=[\n",
    "        (\"variables.named_entity\", \"new york\"),\n",
    "        (\"description\", \"wind\"),\n",
    "    ],\n",
    "    temporal_coverage_start=\"2018-01-01\",\n",
    "    temporal_coverage_end=\"2018-01-03\"\n",
    ")\n",
    "\n",
    "print(len(metadatas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[125530000, 125480000, 124480000, 125450000, 125150000, 124530000, 124620000, 124600000, 125030000]\n"
     ]
    }
   ],
   "source": [
    "print([x[\"_source\"][\"datamart_id\"] for x in metadatas])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Materialize them with their metadata. Constrain should come from UI, will have UI for user to form such constrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\nTraceback (most recent call last):\n  File \"/Users/dongyuli/miniconda3/envs/datamart_env/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n    self.run()\n  File \"/Users/dongyuli/isi/repos/datamart/datamart/utilities/timeout.py\", line 28, in __run\n    self.__run_backup()\n  File \"/Users/dongyuli/miniconda3/envs/datamart_env/lib/python3.6/threading.py\", line 864, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/dongyuli/isi/repos/datamart/datamart/utilities/timeout.py\", line 59, in _new_func\n    result.append(oldfunc(*oldfunc_args, **oldfunc_kwargs))\n  File \"/Users/dongyuli/isi/repos/datamart/datamart/utilities/utils.py\", line 126, in materialize\n    df = materializer.get(metadata=metadata, constrains=constrains)\n  File \"/Users/dongyuli/isi/repos/datamart/datamart/utilities/../materializers/noaa_materializer.py\", line 61, in get\n    return self.fetch_data(date_range=date_range, locations=locations, data_type=data_type, dataset_id=dataset_id)\n  File \"/Users/dongyuli/isi/repos/datamart/datamart/utilities/../materializers/noaa_materializer.py\", line 92, in fetch_data\n    stationid = self.get_available_station(location_id, data_type, dataset_id, start_date, end_date)\n  File \"/Users/dongyuli/isi/repos/datamart/datamart/utilities/../materializers/noaa_materializer.py\", line 158, in get_available_station\n    data = response.json()\n  File \"/Users/dongyuli/miniconda3/envs/datamart_env/lib/python3.6/site-packages/requests/models.py\", line 897, in json\n    return complexjson.loads(self.text, **kwargs)\n  File \"/Users/dongyuli/miniconda3/envs/datamart_env/lib/python3.6/json/__init__.py\", line 354, in loads\n    return _default_decoder.decode(s)\n  File \"/Users/dongyuli/miniconda3/envs/datamart_env/lib/python3.6/json/decoder.py\", line 339, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"/Users/dongyuli/miniconda3/envs/datamart_env/lib/python3.6/json/decoder.py\", line 357, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'infer_objects'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-90d8a6426c8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \"date_range\": {\n\u001b[1;32m     16\u001b[0m                 \u001b[0;34m\"start\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"2018-01-01T00:00:00\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"2018-01-02T23:00:00\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             }\n\u001b[1;32m     19\u001b[0m         }\n",
      "\u001b[0;32m~/isi/repos/datamart/datamart/utilities/utils.py\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(cls, metadata, variables, constrains)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_columns_for_implicit_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"implicit_variables\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'infer_objects'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "new_dfs = []\n",
    "for hitted in metadatas:\n",
    "    named_entity_column = None\n",
    "    for idx, variable in enumerate(hitted[\"_source\"][\"variables\"]):\n",
    "        if variable.get(\"named_entity\", None):\n",
    "            named_entity_column = idx\n",
    "            break\n",
    "    \n",
    "    df = Utils.get_dataset(\n",
    "        metadata=hitted[\"_source\"],\n",
    "        constrains={\n",
    "            \"named_entity\": {\n",
    "                named_entity_column: [\"new york\"]\n",
    "            },\n",
    "            \"date_range\": {\n",
    "                \"start\": \"2018-01-01T00:00:00\",\n",
    "                \"end\": \"2018-01-02T23:00:00\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    new_dfs.append(df)\n",
    "    print(\"========{}========\".format(hitted[\"_source\"][\"description\"]))\n",
    "    print(df)\n",
    "    \n",
    "    if len(df):\n",
    "        print(\"\\n - try to join with the queried one:\")\n",
    "        joined_df = augment.join(\n",
    "            left_df=old_df,\n",
    "            right_df=df,\n",
    "            left_columns=[[1]],  # date column index of old_df\n",
    "            right_columns=[[0]],  # date column index of new_df\n",
    "            left_metadata=None,\n",
    "            right_metadata=hitted[\"_source\"],\n",
    "            joiner=\"rltk\"\n",
    "        )\n",
    "        print(joined_df)\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We get some datasets related to `wind` from NOAA, how to join with old dataframe?\n",
    "### ISI is working on some join methods\n",
    "### First version rltk joiner is working as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
